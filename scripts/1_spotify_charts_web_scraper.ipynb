{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "casual-living",
   "metadata": {},
   "source": [
    "# Spotify Charts Web Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-springer",
   "metadata": {},
   "source": [
    "This code scrapes the Spotify Charts website, gets the necessary data from the Top 200 list (songs, artists, listen counts, song IDs, and ranks in each country at each date), and creates a separate data file for each country for which the data is available. Based on: https://gist.github.com/hktosun/d4f98488cb8f005214acd12296506f48 and https://medium.com/the-innovation/how-to-scrape-the-most-popular-songs-on-spotify-using-python-8a8979fa6b06."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "signal-level",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from datetime import datetime, timedelta, date\n",
    "from tqdm.notebook import tqdm # added for progress bars\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-custom",
   "metadata": {},
   "source": [
    "It generates a list of dates between Jan 1, 2017 and today in YYYY-MM-DD format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hazardous-soviet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def daterange(start_date, end_date):\n",
    "    for n in range(int((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-person",
   "metadata": {},
   "source": [
    "It creates the list of page links we will get the data from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "julian-prisoner",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://spotifycharts.com/regional/'\n",
    "\n",
    "def create_links(country):\n",
    "    start_date = date(2017, 1, 6) # adjusting the start date\n",
    "    end_date = datetime.today().date()\n",
    "    links = []\n",
    "    dates = list(daterange(start_date, end_date))[::7] # adjusted for weekly rather than daily\n",
    "    for start, end in zip(dates, dates[1:]):\n",
    "        links.append(base_url + country + '/weekly/' + start.strftime('%Y-%m-%d') + '--' + end.strftime('%Y-%m-%d')) # add end date to create interval\n",
    "    return(links, dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-warner",
   "metadata": {},
   "source": [
    "It reads the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "meaning-complex",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_webpage(link):\n",
    "    start = time.time()\n",
    "    page = requests.get(link)\n",
    "    soup = bs(page.content, 'html.parser')\n",
    "    return(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "appropriate-swiss",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [403]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update: 2021-05-2: unfortunately, Spotify seem to have revoked permission for querying the source code of their webpage,\n",
    "# or at least through the \"requests\" library. Moreover, Spotify also have changed the URL, whereas previously\n",
    "# the country was indicated by its full name, it's now indicated by the Alpha-2 country codes.\n",
    "\n",
    "requests.get(\"https://spotifycharts.com/regional/de/weekly/2017-01-06--2017-01-13\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "actual-moldova",
   "metadata": {},
   "source": [
    "It collects the data for each country, and write them in a list. The entries are (in order): Song, Artist, Date, Song ID, Play Count, Rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fabulous-broad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(country):\n",
    "    [links, dates] = create_links(country);\n",
    "    rows = []\n",
    "    for (link, date) in zip(links, tqdm(dates)): # zip added to generate series of tuples\n",
    "        start = time.time()\n",
    "        soup = get_webpage(link)\n",
    "        entries = soup.find_all('td', class_ = 'chart-table-track')\n",
    "        streams = soup.find_all('td', class_ = 'chart-table-streams')\n",
    "        url = soup.find_all('td', class_ = 'chart-table-image') # add url to get song ID\n",
    "        for i, (entry, stream, url) in enumerate(zip(entries, streams, url)): # add url\n",
    "            song = entry.find('strong').get_text()\n",
    "            artist = entry.find('span').get_text()[3:]\n",
    "            songid = url.find('a').get('href') # get url \n",
    "            # songid = songid.split('track/')[1] # split url and extract ID\n",
    "            play_count = stream.get_text()\n",
    "            rows.append([song, artist, date, songid, play_count, i + 1])\n",
    "\n",
    "    return(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-approval",
   "metadata": {},
   "source": [
    "Due to same connection error the scraping had to start over a couple of times. To not start from stratch a list was build containing the concluded countries. Names have to be transformed to ISO-2 codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "round-meditation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# due to connection issues, the scraping had to start several times over. this tuple indicted the scraped countries\n",
    "# such that starting from scratch was not neccessary.\n",
    "\n",
    "done = [] # insert country names when done\n",
    "\n",
    "import country_converter as coco\n",
    "iso2_codes = coco.convert(names = done, to = 'ISO2', not_found = None)\n",
    "iso2_codes_lower = [x.lower() for x in iso2_codes]\n",
    "\n",
    "# Spotify offers the global charts as well which are obviously not found by the country_converted. warning can be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-opera",
   "metadata": {},
   "source": [
    "It exports the data for each country in a csv format. The column names are Song, Artist, Song ID, Date, Streams, Rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fewer-reflection",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/1_charts_per_country/'\n",
    "\n",
    "def save_data(country):\n",
    "    if not os.path.exists('data'):\n",
    "        os.makedirs('data')\n",
    "    file_name = file_path + country[1].replace(' ', '_').lower() + '.csv'\n",
    "    if country[0] not in iso2_codes_lower:\n",
    "        print(country[0])\n",
    "        data = get_data(country[0])\n",
    "        if(len(data) != 0):\n",
    "            data = pd.DataFrame(data, columns = ['Song', 'Artist', 'Date', 'Song ID', 'Streams', 'Rank']) # add 'Song ID'\n",
    "            data.to_csv(file_name, sep = ',', float_format = '%s', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-vaccine",
   "metadata": {},
   "source": [
    "It generates a list of countries for which the data is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "lesbian-mirror",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_countries():\n",
    "    page = requests.get(base_url)\n",
    "    soup = bs(page.content, 'html.parser')\n",
    "    countries = []\n",
    "    ctys = soup.find('ul').findAll('li')\n",
    "    for cty in ctys:\n",
    "        countries.append([cty[\"data-value\"], cty.get_text()])\n",
    "    return(countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-average",
   "metadata": {},
   "source": [
    "It runs the function save_data for each country. In other words, it creates the .csv data files for each country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "molecular-polyester",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'findAll'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e74326ac9ee4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msave_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mscrape_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-e74326ac9ee4>\u001b[0m in \u001b[0;36mscrape_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mscrape_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mcountries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_countries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcountry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msave_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-38afe6facf18>\u001b[0m in \u001b[0;36mget_countries\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcountries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mctys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ul'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'li'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcty\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mctys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mcountries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcty\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data-value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'findAll'"
     ]
    }
   ],
   "source": [
    "def scrape_data():\n",
    "    countries = get_countries()\n",
    "    for country in tqdm(countries):\n",
    "        save_data(country)\n",
    "\n",
    "scrape_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organizational-recording",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
